<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Making an MLP in Rust, From Scratch, for MNIST on Max&#39;s blurtie blog</title>
    <link>//localhost:1313/series/making-an-mlp-in-rust-from-scratch-for-mnist/</link>
    <description>Recent content in Making an MLP in Rust, From Scratch, for MNIST on Max&#39;s blurtie blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Aug 2025 10:45:32 +0100</lastBuildDate>
    <atom:link href="//localhost:1313/series/making-an-mlp-in-rust-from-scratch-for-mnist/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The maths behind the MLP</title>
      <link>//localhost:1313/posts/the_maths_behind_the_mlp/</link>
      <pubDate>Mon, 11 Aug 2025 10:45:32 +0100</pubDate>
      <guid>//localhost:1313/posts/the_maths_behind_the_mlp/</guid>
      <description>&lt;p&gt;This blog(/tutorial maybe?) is the first part in a walk-through of the process I followed to build a multi-layer-perceptron (MLP) from scratch in rust. Followed by using it to classify the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34;&gt;MNIST dataset&lt;/a&gt;. The source code for the MLP can be found &lt;a href=&#34;https://github.com/max-amb/number_recognition&#34;&gt;here&lt;/a&gt;.&#xA;This means no &lt;a href=&#34;https://github.com/LaurentMazare/tch-rs&#34;&gt;pytorch&lt;/a&gt; or &lt;a href=&#34;https://github.com/tensorflow/rust&#34;&gt;tensorflow&lt;/a&gt;, both of which have rust bindings, just the rust standard library and the beautiful linear algebra crate, &lt;a href=&#34;https://nalgebra.rs/&#34;&gt;nalgebra&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h1 id=&#34;why-from-scratch&#34;&gt;Why from scratch&lt;/h1&gt;&#xA;&lt;p&gt;I am not entirely sure. Initially, I watched the first four episodes in the excellent &lt;a href=&#34;https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;amp;si=VXW032Kq09cHEgpa&#34;&gt;3blue1brown series on neural networks&lt;/a&gt;.&#xA;This piqued my interest and as I had an abundance of free time (having just finished my leaving school exams) I decided to implement a MLP for MNIST like the videos prescribed.&#xA;I considered using the previously discussed machine learning libraries, but they felt a bit like cheating and there was something (perhaps deceivingly) alluring about implementing the maths behind machine learning.&#xA;So, I went onwards.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
