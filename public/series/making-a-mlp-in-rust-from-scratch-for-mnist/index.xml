<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Making a MLP, in rust, from scratch, for MNIST on Max&#39;s blurtie blog</title>
    <link>//localhost:1313/series/making-a-mlp-in-rust-from-scratch-for-mnist/</link>
    <description>Recent content in Making a MLP, in rust, from scratch, for MNIST on Max&#39;s blurtie blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/series/making-a-mlp-in-rust-from-scratch-for-mnist/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The maths behind the MLP</title>
      <link>//localhost:1313/posts/the_maths_behind_the_mlp/</link>
      <pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/the_maths_behind_the_mlp/</guid>
      <description>&lt;nav id=&#34;TableOfContents&#34;&gt;&#xA;  &lt;ul&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#why-from-scratch&#34;&gt;Why from scratch&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#intuition&#34;&gt;Intuition?&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#notation&#34;&gt;Notation&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#forward-pass&#34;&gt;Forward pass&lt;/a&gt;&#xA;      &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#matrix-notation&#34;&gt;Matrix notation&lt;/a&gt;&#xA;          &lt;ul&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#vector-of-z-values&#34;&gt;Vector of z values&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#vector-of-neuron-values&#34;&gt;Vector of neuron values&lt;/a&gt;&#xA;              &lt;ul&gt;&#xA;                &lt;li&gt;&lt;a href=&#34;#non-linearity&#34;&gt;Non-linearity&lt;/a&gt;&lt;/li&gt;&#xA;              &lt;/ul&gt;&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&#xA;          &lt;/ul&gt;&#xA;        &lt;/li&gt;&#xA;      &lt;/ul&gt;&#xA;    &lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#backpropagation&#34;&gt;Backpropagation&lt;/a&gt;&#xA;      &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#output-layer-derivatives&#34;&gt;Output layer derivatives&lt;/a&gt;&#xA;          &lt;ul&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#bias-derivative&#34;&gt;Bias derivative&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#weight-derivative&#34;&gt;Weight derivative&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#delta&#34;&gt;Delta&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#representation-of-output-layer-backpropagation-as-a-matrix-operation&#34;&gt;Representation of output layer backpropagation as a matrix operation&lt;/a&gt;&#xA;              &lt;ul&gt;&#xA;                &lt;li&gt;&lt;a href=&#34;#added-notation&#34;&gt;Added notation&lt;/a&gt;&lt;/li&gt;&#xA;              &lt;/ul&gt;&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#conclusion-of-the-output-layer-derivatives&#34;&gt;Conclusion of the output layer derivatives&lt;/a&gt;&lt;/li&gt;&#xA;          &lt;/ul&gt;&#xA;        &lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#hidden-layer-derivatives&#34;&gt;Hidden layer derivatives&lt;/a&gt;&#xA;          &lt;ul&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#intuition-behind-the-derivative-sum&#34;&gt;Intuition behind the derivative sum&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#derivative-of-the-bias&#34;&gt;Derivative of the bias&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#derivative-of-the-weight&#34;&gt;Derivative of the weight&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#deltas-again&#34;&gt;Delta&amp;rsquo;s again&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#representation-of-hidden-layer-backpropagation-as-a-matrix-operation&#34;&gt;Representation of hidden layer backpropagation as a matrix operation&lt;/a&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;#conclusion-of-the-hidden-layer-derivatives&#34;&gt;Conclusion of the hidden layer derivatives&lt;/a&gt;&lt;/li&gt;&#xA;          &lt;/ul&gt;&#xA;        &lt;/li&gt;&#xA;      &lt;/ul&gt;&#xA;    &lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/nav&gt;&#xA;&#xA;&lt;p&gt;This blog(/tutorial maybe?) is the first part in a walk-through of the process I followed to build a multi-layer-perceptron (MLP) from scratch in rust. Followed by using it to classify the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34;&gt;MNIST dataset&lt;/a&gt;. The source code for the MLP can be found &lt;a href=&#34;https://github.com/max-amb/number_recognition&#34;&gt;here&lt;/a&gt;.&#xA;This means no &lt;a href=&#34;https://github.com/LaurentMazare/tch-rs&#34;&gt;pytorch&lt;/a&gt; or &lt;a href=&#34;https://github.com/tensorflow/rust&#34;&gt;tensorflow&lt;/a&gt;, both of which have rust bindings, just the rust standard library and the beautiful linear algebra crate, &lt;a href=&#34;https://nalgebra.rs/&#34;&gt;nalgebra&lt;/a&gt;.&#xA;If you just want to get the final equations and be done (which I do not recommend), see the &lt;a href=&#34;#summary&#34;&gt;summary&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
